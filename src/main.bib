% this is the bibliography file

@Misc{iso-odf,
  author = 	 {International Organization for Standardization},
  title = 	 {ISO/IEC 26300:2006 Open Document Format},
  howpublished = {\url{http://std.dkuug.dk/keld/iso26300-odf/is26300/iso_iec_26300:2006_e.pdf}},
  month = 	 {December},
  year = 	 {2006},
  annote = 	 {\url{http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=43485}}
}

@inproceedings{Auer2007,
abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
author = {Auer, S\"{o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-76298-0\_52},
isbn = {3540762973},
issn = {03029743},
pages = {722--735},
title = {{DBpedia: A nucleus for a Web of open data}},
url = {http://jenslehmann.org/files/2008/dbpedia.pdf},
volume = {4825 LNCS},
year = {2007}
}

@misc{dbpedia,
title = {{DBpedia}},
url = {dbpedia.org}
}

@article{Finlayson2013,
author = {Finlayson, Mark Alan},
file = {:home/adrian/current/licenta/bibliography/JWI\_WordNet.pdf:pdf},
isbn = {9789949324927},
journal = {the 7th Global Wordnet Conference},
title = {{Code for Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation}},
url = {http://dspace.mit.edu/handle/1721.1/81949},
year = {2013}
}

@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George a.},
doi = {10.1145/219717.219748},
file = {:home/adrian/current/licenta/bibliography/1995 - Miller - WordNet a lexical database for English.pdf:pdf},
isbn = {1558602720},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
url = {http://friendica.biome.tk/20140731/Reference/1995 - Miller - WordNet a lexical database for English.pdf},
volume = {38},
year = {1995}
}

@article{Wilcox2011,
author = {Wilcox, Bruce},
file = {:home/adrian/current/licenta/bibliography/Pattern\_Matching\_for\_Natural\_Language\_Applications.pdf:pdf},
journal = {Gamasutra},
keywords = {3d animation,3d modeling,3d studio max textures,3d technology,arcade development,artificial intelligence,cmp game media group,computer game developers conference,digital assets,digital entertainment,dreamcast development,free 3d models,free shaders,free textures,gamasutra exchange,game animation,game audio,game business,game design,game developer,game developer magazine,game developers conference,game development,game development software,game directory,game industry research,game jobs,game news,game producer,game programmer,game programming,game technology,independent game developers conference,new game,nintendo development,online game development,pc game,playstation 2,playstation development,ps2,videogame,virtual reality,xbox game},
pages = {1--5},
title = {{Beyond Fa\c{c}ade: Pattern Matching for Natural Language Applications}},
url = {http://www.gamasutra.com/view/feature/6305/beyond\_fa\c{c}ade\_pattern\_matching\_.php?page=1},
year = {2011}
}

@article{Manning2014,
author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
file = {:home/adrian/current/licenta/bibliography/acl2014-corenlp.pdf:pdf},
journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
pages = {55--60},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://www.aclweb.org/anthology/P/P14/P14-5010},
year = {2014}
}

@article{Turing1950,
author = {Turing, Alan},
file = {:home/adrian/pub/4/2/licenta/bibliography/COMPUTING-MACHINERY-AND-INTELLIGENCE-Turing.pdf:pdf},
issn = {0026-4423},
journal = {Mind},
number = {236},
pages = {433--460},
title = {{Computing machinery and intelligence}},
url = {papers2://publication/uuid/E74CAAC6-F3DD-47E7-AEA6-5FB511730877},
volume = {59},
year = {1950}
}

@article{Lehmann2012,
abstract = {The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on theWeb using SemanticWeb and Linked Data technologies. The project extracts knowledge from 111 different language editions ofWikipedia. The largest DBpedia knowledge base which is extracted from the English edition ofWikipedia consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other 110Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties. The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the differentWikipedia editions to be combined. The project publishes regular releases of all DBpedia knowledge bases for download and provides SPARQL query access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases, the project maintains a live knowledge base which is updated whenever a page inWikipedia changes. DBpedia sets 27 million RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia data. Several hundred data sets on theWeb publish RDF links pointing to DBpedia themselves and thus make DBpedia one of the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and applications. Keywords:},
author = {Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mende, Pablo N. and Hellmann, Sebastian and Morsey, Mohamed and van Kleef, Patrick and Auer, Soren and Bizer, Christian},
doi = {10.3233/SW-140134},
file = {:home/adrian/current/licenta/bibliography/DBpedia2014.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
keywords = {knowledge extraction,linked data,multilingual knowledge bases,rdf,wikipedia},
pages = {1--5},
title = {{DBpedia – A Large-scale , Multilingual Knowledge Base Extracted from Wikipedia}},
url = {http://svn.aksw.org/papers/2013/SWJ\_DBpedia/public.pdf},
volume = {1},
year = {2012}
}

@article{Miller1990,
abstract = {WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.},
author = {Miller, George A. and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine J.},
doi = {10.1093/ijl/3.4.235},
isbn = {0950384614774577},
issn = {09503846},
journal = {International Journal of Lexicography},
number = {4},
pages = {235--244},
pmid = {15102489},
title = {{Introduction to wordnet: An on-line lexical database}},
volume = {3},
year = {1990}
}

@book{Fellbaum1998,
abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
author = {Fellbaum, Christiane},
booktitle = {British Journal Of Hospital Medicine London England 2005},
doi = {10.1139/h11-025},
isbn = {026206197X},
issn = {17508460},
number = {3},
pages = {423},
pmid = {21561289},
title = {{WordNet: An Electronic Lexical Database}},
url = {http://acl.ldc.upenn.edu/J/J99/J99-2008.pdf},
volume = {71},
year = {1998}
}

@article{Finlayson2011,
abstract = {JWI (the MIT Java Wordnet Interface) is a Java library for interfacing with Wordnet. JWI supports access to Wordnet versions 1.6 through 3.0, among other related Wordnet extensions. Wordnet is a freely and publicly available semantic dictionary of English, developed at Princeton University. JWI is written for Java 1.5.0 and has the package namespace edu.mit.jwi. The distribution does not include the Wordnet dictionary files; these can be downloaded from the Wordnet download site. This version of software is distributed under a license that makes it free to use for all purposes, as long as proper copyright acknowledgement is made. The javadoc API is posted online for your convenience. So is the version changelog. If you find JWI useful, have found a bug, or would like to request a new feature, please contact me.},
author = {Finlayson, Mark a.},
file = {:home/adrian/current/licenta/bibliography/edu.mit.jwi\_2.3.3\_manual.pdf:pdf},
pages = {1--10},
title = {{MIT Java Wordnet Interface (JWI) User's Guide}},
year = {2011}
}

@book{VanRijsbergen1979,
abstract = {Information retrieval is a wide, often loosely-defined term but in these pages I shall be concerned only with automatic information retrieval systems. Automatic as opposed to manual and information as opposed to data or fact. Unfortunately the word information can be very misleading. In the context of information retrieval (IR), information, in the technical meaning given in Shannon's theory of communication, is not readily measured (Shannon and Weaver1). In fact, in many cases one can...},
author = {{Van Rijsbergen}, C J},
booktitle = {Butterworths},
title = {{Information Retrieval, 2nd edition}},
url = {citeseer.ist.psu.edu/vanrijsbergen79information.html$\backslash$nhttp://www.dcs.gla.ac.uk/Keith/Preface.html},
year = {1979}
}

@article{Biaecki2012,
abstract = {Apache Lucene is a modern, open source search library designed to provide both relevant results as well as high performance. Furthermore, Lucene has undergone significant change over the years, starting as a one - person project to one of the leading search solutions available. Lucene is used in a vast range of applications from mobile devic es and desktops through Internet scale solutions. The evolution of Lucene has been quite dramatic at times, none more so than in the current release of Lucene 4.0. This paper presents both an overview of Lucene’s features as well as details on its commun ity development model, architecture and implementation, including coverage of its indexing and scoring capabilities.},
author = {Białecki, Andrzej and Muri, Robert and Ingersoll, Grant},
file = {:home/adrian/current/licenta/bibliography/lucene\_architecture.pdf:pdf},
isbn = {978-0-473-22026-6},
journal = {Proceedings of the SIGIR 2012 Workshop on Open Source Information Retrieval},
pages = {17--24},
title = {{Apache Lucene 4}},
url = {http://www.cs.otago.ac.nz/homepages/andrew/involvement/2012-SIGIR-OSIR.pdf?origin=publication\_detail\#page=22},
year = {2012}
}

@misc{StanfordTokenizer,
keywords = {StanfordTokenizer},
mendeley-tags = {StanfordTokenizer},
title = {{Stanford Tokenizer Documentation}},
howpublished={\url{http://nlp.stanford.edu/software/tokenizer.shtml}},
note= {Accessed: 2015-06-26},
url = {http://nlp.stanford.edu/software/tokenizer.shtml}
}

@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24\% accuracy on the Penn Treebank WSJ, an error reduction of 4.4\% on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
doi = {10.3115/1073445.1073478},
file = {:home/adrian/current/licenta/bibliography/stanford/tagging.pdf:pdf},
journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
pages = {252--259},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://dl.acm.org/citation.cfm?id=1073478},
year = {2003}
}

@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models. In this paper, we review our experience with constructing one such large annotated corpus-the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcus, Santorini, Marcinkiewicz - 1993 - Building a Large Annotated Corpus of English The Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {2},
pages = {313--330},
title = {{Building a Large Annotated Corpus of English: The Penn Treebank}},
url = {http://portal.acm.org/citation.cfm?id=972470.972475},
volume = {19},
year = {1993}
}

@article{Finkel2005,
abstract = {Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, andCRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9\% over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:home/adrian/current/licenta/bibliography/stanford/ner.pdf:pdf},
journal = {in Acl},
number = {1995},
pages = {363 -- 370},
title = {{Incorporating non-local information into information extraction systems by gibbs sampling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.8904},
year = {2005}
}

@article{Lee2013,
abstract = {We propose a new deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model's cluster output. The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision oriented, offer a powerful way to achieve both high precision and high recall. Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity. Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics.},
author = {Lee, Heeyoung and Chang, Angel and Peirsman, Yves and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan},
doi = {10.1162/COLI\_a\_00152},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2013 - Deterministic coreference resolution based on entity-centric, precision-ranked rules.pdf:pdf;:home/adrian/current/licenta/bibliography/stanford/dcoref.pdf:pdf},
isbn = {9781608459858},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {4},
pages = {1--54},
title = {{Deterministic coreference resolution based on entity-centric, precision-ranked rules}},
url = {http://dx.doi.org/10.1162/COLI\_a\_00152$\backslash$nhttp://www.mitpressjournals.org/doi/pdf/10.1162/COLI\_a\_00152$\backslash$nhttp://www.mitpressjournals.org/toc/coli/0/ja$\backslash$nhttp://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00152},
volume = {39},
year = {2013}
}

@article{Weizenbaum1966,
abstract = {ELIZA is a program operating within the MAC time-sharing system at MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA "scripts". A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.},
author = {Weizenbaum, Joseph},
doi = {10.5100/jje.2.3\_1},
isbn = {0001-0782},
issn = {0549-4974},
journal = {Communications of the ACM},
number = {1},
pages = {36--45},
pmid = {12345678},
title = {{ELIZA — A Computer Program For the Study of Natural Language Communication Between Man And Machine}},
url = {http://joi.jlc.jst.go.jp/JST.Journalarchive/jje1965/2.3\_1?from=CrossRef},
volume = {9},
year = {1966}
}

@incollection{Wallace2009,
abstract = {{This paper is a technical presentation of Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) and Artificial Intelligence Markup Language (AIML), set in context by historical and philosophical ruminations on human consciousness. A.L.I.C.E\}, the first AIML-based personality program, won the Loebner Prize as “the most human computer” at the annual Turing Test contests in 2000, 2001, and 2004. The program, and the organization that develops it, is a product of the world of free software. More than 500 volunteers from around the world have contributed to her development. This paper describes the history of A.L.I.C.E. and AIML-free software since 1995, noting that the theme and strategy of deception and pretense upon which AIML is based can be traced through the history of Artificial Intelligence research. This paper goes on to show how to use AIML to create robot personalities like A.L.I.C.E. that pretend to be intelligent and selfaware. The paper winds up with a survey of some of the philosophical literature on the question of consciousness. We consider Searle’s Chinese Room, and the view that natural language understanding by a computer is impossible. We note that the proposition “consciousness is an illusion” may be undermined by the paradoxes it apparently implies. We conclude that A.L.I.C.E. does pass the Turing Test, at least, to paraphrase Abraham Lincoln, for some of the people some of the time.},
author = {Wallace, Richard S.},
booktitle = {Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer},
doi = {10.1007/978-1-4020-6710-5\_13},
isbn = {9781402067105},
keywords = {A.L.I.C.E,Artificial Intelligence,Artificial Intelligence Markup Language (AIML),Artificial Linguistic Internet Computer Entity,HTML,Loebner Prize,Markup Languages,Turing Test,XML,behaviorism,bot,chat robot,consciousness,deception,dualism,free software,natural language,open source,philosophy of mind,recursion,stimulusresponse,targeting},
pages = {181--210},
title = {{The anatomy of A.L.I.C.E.}},
year = {2009}
}

@article{Heller2005,
abstract = {A chatbot named Freudbot was constructed using the open source architecture of AIML to determine if a famous person application of chatbot technology could improve student-content interaction in distance education. Fifty-three students in psychology completed a study in which they chatted with Freudbot over the web for 10 minutes under one of two instructional sets. They then completed a questionnaire to provide information about their experience and demographic variables. The results from the questionnaire indicated a neutral evaluation of the chat experience although participants positively endorsed the expansion of chatbot technology and provided clear direction for future development and improvement. A basic analysis of the chatlogs indicated a high proportion of on-task behaviour. There was no effect of instructional set. Altogether, the findings indicate that famous person applications of chatbot technology may be promising as a teaching and learning tool in distance and online education.},
author = {Heller, Bob and Procter, Mike and Mah, Dean},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heller, Procter, Mah - 2005 - Freudbot An investigation of chatbot technology in distance education.pdf:pdf},
journal = {Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications},
keywords = {Distance Education,Educational Technology},
pages = {3913--3918},
title = {{Freudbot: An investigation of chatbot technology in distance education}},
url = {http://www.editlib.org/index.cfm?fuseaction=Reader.ViewFullText\&paper\_id=20691},
year = {2005}
}

@misc{Ferrucci2010,
abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.},
author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A. and Lally, Adam and Murdock, J. William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Chris},
booktitle = {AI Magazine},
doi = {10.1609/aimag.v31i3.2303},
isbn = {9781450301787},
issn = {0738-4602},
number = {3},
pages = {59--79},
title = {{Building Watson: An Overview of the DeepQA Project}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2303},
volume = {31},
year = {2010}
}

@article{Chowdhury2003,
abstract = {Natural Language Processing (NLP) is an area of research and application that explores how computers can be used to understand and manipulate natural language text or speech to do useful things. NLP researchers aim to gather knowledge on how human beings understand and use language so that appropriate tools and techniques can be developed to make computer systems understand and manipulate natural languages to perform the desired tasks. The foundations of NLP lie in a number of disciplines, viz. computer and information sciences, linguistics, mathematics, electrical and electronic engineering, artificial intelligence and robotics, psychology, etc. Applications of NLP include a number of fields of studies, such as machine translation, natural language text processing and summarization, user interfaces, multilingual and cross language information retrieval (CLIR), speech recognition, artificial intelligence and expert systems, and so on. One important area of application of NLP that is relatively new and has not been covered in the previous ARIST chapters on NLP has become quite prominent due to the proliferation of the world wide web and digital libraries. Several researchers have pointed out the need for appropriate research in facilitating multi- or cross-lingual information retrieval, including multilingual text processing and multilingual user interface systems, in order to exploit the full benefit of the www and digital libraries (see for example, Borgman, 1997; Peters \& Picchi, 1997)},
author = {Chowdhury, Gobinda G.},
doi = {10.1002/aris.1440370103},
isbn = {1550-8382},
issn = {1550-8382},
journal = {Annual Review of Information Science and Technology},
number = {1},
pages = {51--89},
title = {{Natural language processing}},
volume = {37},
year = {2003}
}

@article{Burger2001,
abstract = {Recently the Vision Statement to Guide Research in Question Answering Q\&A and Text Summarization outlined a deliberately ambitious vision for research in Q\&A. This vision is a challenge to the Roadmap Committee to define the program structures capable of addressing the question processing and answer extraction subtasks and combine them in increasingly sophisticated ways, such that the vision for research is made possible.},
author = {Burger, John and Cardie, Claire and Chaudhri, Vinay and Gaizauskas, Robert and Harabagiu, Sanda and Israel, David and Jacquemin, Christian and Lin, C Y and Maiorano, Steve and Miller, George and {Et Al.}},
journal = {New York},
pages = {1--35},
title = {{Issues , Tasks and Program Structures to Roadmap Research in Question \& Answering ( Q \& A )}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Issues,+Tasks+and+Program+Structures+to+Roadmap+Research+in+Question+\&+Answering+(Q\&A)\#0},
year = {2001}
}

@article{Haller2013,
abstract = {There are many applications that are incorporating a human appearance and intending to simulate human dialog, but in most of the cases the knowledge of the conversational bot is stored in a database created by a human experts. However, very few researches have investigated the idea of creating a chat-bot with an artificial character and personality starting from web pages or plain text about a certain person. This paper describes an approach to the idea of identifying the most important facts in texts describing the life (including the personality) of an historical figure for building a conversational agent that could be used in middle-school CSCL scenarios.},
author = {Haller, Emanuela and Rebedea, Traian},
doi = {10.1109/CSCS.2013.85},
file = {:home/adrian/pub/4/2/licenta/bibliography/Designing-a-Chat-bot-that-Simulates-an-Historical-Figure.pdf:pdf},
isbn = {978-1-4673-6140-8},
journal = {Proceedings - 19th International Conference on Control Systems and Computer Science, CSCS 2013},
keywords = {Chat-bot,Conversational Agent,Information Extraction,Parsing,Wikipedia},
pages = {582--589},
title = {{Designing a chat-bot that simulates an historical figure}},
year = {2013}
}

@article{Bogatu,
author = {Bogatu, Adrian and Rotărescu, Dorin and Rebedea, Traian},
file = {:home/adrian/current/licenta/bibliography/Conversational Agent that Models a Historical Personality.pdf:pdf},
title = {{Conversational Agent that Models a Historical Personality}},
year = {{\em In press}}
}

@incollection{Taylor2007,
abstract = {Control over a closed domain of textual material removes many question answering issues, as does an ontology that is closely intertwined with its sources. This pragmatic, shallow approach to many challenging areas of research in adaptive hypermedia, question answering, intelligent tutoring and humancomputer interaction has been put into practice at Cambridge in the Computer Science undergraduate course to teach the hardware description language Veri/og. This language itself poses many challenges as it crosses the interdisciplinary boundary between hardware and software engineers, giving rise to severalhuman ontologies as well as theprogramming language itself We present further results from ourformal and informal surveys. We look at further work to increase the dialogue between studentand tutor and export our knowledge to the Semantic Web.},
author = {Taylor, Kate and Moore, Simon},
booktitle = {Applications and Innovations in Intelligent Systems XIV},
doi = {10.1007/978-1-84628-666-7},
isbn = {978-1-84628-665-0},
pages = {193--206},
title = {{Adding question answering to an e-tutor for programming languages}},
url = {http://www.springerlink.com/index/10.1007/978-1-84628-666-7},
year = {2007}
}

@article{Giunchiglia2004,
author = {Giunchiglia, Fausto and Shvaiko, Pavel},
file = {:home/adrian/current/licenta/bibliography/Semantic Matching.pdf:pdf},
issn = {1469-8005},
journal = {Knowledge Engineering Review journal},
number = {3},
pages = {265--280},
title = {{Semantic Matching}},
volume = {18},
year = {2004}
}

@article{Giunchiglia2007,
author = {Giunchiglia, Fausto and Yatskevich, Mikalai and Shvaiko, Pavel},
file = {:home/adrian/current/licenta/bibliography/Semantic Matching$\backslash$: Algorithms and Implementation.pdf:pdf},
journal = {Journal on Data Semantics},
title = {{Semantic matching: Algorithms and implementation}},
url = {http://www.springerlink.com/index/8w85238007kg82q8.pdf},
year = {2007}
}

@article{Yih2013,
author = {Yih, Wen-tau and Chang, Ming-Wei and Meek, Christopher and Pastusiak, Andrzej},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yih et al. - 2013 - Question Answering Using Enhanced Lexical Semantic Models.pdf:pdf},
journal = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {1744--1753},
title = {{Question Answering Using Enhanced Lexical Semantic Models}},
url = {http://www.aclweb.org/anthology/P13-1171},
year = {2013}
}

@article{Moritz,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.1632v1},
author = {Moritz, Karl and Phil, Hermann and Stephen, Blunsom},
eprint = {arXiv:1412.1632v1},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moritz, Phil, Stephen - Unknown - Deep Learning for Answer Sentence Selection.pdf:pdf},
pages = {1--9},
title = {{Deep Learning for Answer Sentence Selection}}
}

@article{Echihabi2006,
author = {Echihabi, A. and Hermjakob, U. and Hovy, E. and Marcu, D. and Melz, E. and Ravichandran, D.},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Echihabi et al. - 2006 - How to Select an Answer String.pdf:pdf},
journal = {Advances in open domain question answering},
pages = {383--406},
title = {{How to Select an Answer String?}},
url = {http://www.springerlink.com/index/WQJGR9P0W23007W6.pdf},
year = {2006}
}

@article{Kerry2009,
abstract = {This paper discusses the use of natural language or ‘conversational’ agents in e-learning environments. We describe and contrast the various applications of conversational agent technology represented in the e-learning literature, including tutors, learning companions, language practice and systems to encourage reflection. We offer two more detailed examples of conversational agents, one which provides learning support, and the other support for self-assessment. Issues and challenges for developers of conversational agent systems for e-learning are identified and discussed.},
author = {Kerry, Alice and Ellis, Richard and Bull, Susan},
doi = {10.1007/978-1-84882-215-3-13},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kerry, Ellis, Bull - 2009 - Conversational agents in e-learning.pdf:pdf},
isbn = {9781848822146},
journal = {Applications and Innovations in Intelligent Systems XVI - Proceedings of AI 2008, the 28th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence},
pages = {169--182},
title = {{Conversational agents in e-learning}},
year = {2009}
}

@article{Kopp2005,
abstract = {This paper describes an application of the conversational agent Max in a real-world setting. The agent is employed as guide in a public computer museum, where he engages with visitors in natural face-to-face communication, provides them with information about the museum or the exhibition, and con- ducts natural small talk conversations. The design of the system is described with a focus on how the conversational behavior is achieved. Logfiles from in- teractions between Max and museum visitors were analyzed for the kinds of dialogue people are willing to have with Max. Results indicate that Max en- gages people in interactions where they are likely to use human-like communi- cation strategies, suggesting the attribution of sociality to the agent.},
author = {Kopp, Stefan and Gesellensetter, Lars and Kr\"{a}mer, Nicole C. and Wachsmuth, Ipke},
doi = {10.1007/11550617\_28},
file = {:home/adrian/current/licenta/bibliography/related-work/museumguide.pdf:pdf},
isbn = {3540287388},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {329--343},
title = {{A conversational agent as museum guide - Design and evaluation of a real-world application}},
volume = {3661 LNAI},
year = {2005}
}

@article{Mehta2007,
abstract = {We report on the benefits achieved by using ontologies in the context of a fully implemented conversational system that allows for a real-time rich communication between primarily 10 to 18 years old human users and a 3D graphical character through spontaneous speech and gesture. In this paper, we focus on the categorization of ontological resources into domain independent and domain specific components in the effort of both augmenting the agent's conversational capabilities and enhancing system's reusability across conversational domains. We also present a novel method of exploiting the existing ontological resources along with Google directory categorization for a semi-automatic understanding of user utterance on general purpose topics like e.g. movies and games.},
author = {Mehta, Manish and Corradini, Andrea},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehta, Corradini - 2007 - Developing a Conversational Agent Using Ontologies.pdf:pdf},
isbn = {978-3-540-73108-5},
issn = {03029743},
journal = {HCI'07 Proceedings of the 12th international conference on Human-computer interaction: intelligent multimodal interaction environments},
pages = {154--164},
title = {{Developing a Conversational Agent Using Ontologies}},
url = {http://dl.acm.org/citation.cfm?id=1769608},
year = {2007}
}

