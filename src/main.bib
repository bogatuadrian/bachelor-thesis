% this is the bibliography file

@Misc{iso-odf,
  author = 	 {International Organization for Standardization},
  title = 	 {ISO/IEC 26300:2006 Open Document Format},
  howpublished = {\url{http://std.dkuug.dk/keld/iso26300-odf/is26300/iso_iec_26300:2006_e.pdf}},
  month = 	 {December},
  year = 	 {2006},
  annote = 	 {\url{http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=43485}}
}

@inproceedings{Auer2007,
abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
author = {Auer, S\"{o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-76298-0\_52},
isbn = {3540762973},
issn = {03029743},
pages = {722--735},
title = {{DBpedia: A nucleus for a Web of open data}},
url = {http://jenslehmann.org/files/2008/dbpedia.pdf},
volume = {4825 LNCS},
year = {2007}
}

@misc{dbpedia,
title = {{DBpedia}},
url = {dbpedia.org}
}

@article{Finlayson2013,
author = {Finlayson, Mark Alan},
file = {:home/adrian/current/licenta/bibliography/JWI\_WordNet.pdf:pdf},
isbn = {9789949324927},
journal = {the 7th Global Wordnet Conference},
title = {{Code for Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation}},
url = {http://dspace.mit.edu/handle/1721.1/81949},
year = {2013}
}

@article{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George a.},
doi = {10.1145/219717.219748},
file = {:home/adrian/current/licenta/bibliography/1995 - Miller - WordNet a lexical database for English.pdf:pdf},
isbn = {1558602720},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
url = {http://friendica.biome.tk/20140731/Reference/1995 - Miller - WordNet a lexical database for English.pdf},
volume = {38},
year = {1995}
}

@article{Wilcox2011,
author = {Wilcox, Bruce},
file = {:home/adrian/current/licenta/bibliography/Pattern\_Matching\_for\_Natural\_Language\_Applications.pdf:pdf},
journal = {Gamasutra},
keywords = {3d animation,3d modeling,3d studio max textures,3d technology,arcade development,artificial intelligence,cmp game media group,computer game developers conference,digital assets,digital entertainment,dreamcast development,free 3d models,free shaders,free textures,gamasutra exchange,game animation,game audio,game business,game design,game developer,game developer magazine,game developers conference,game development,game development software,game directory,game industry research,game jobs,game news,game producer,game programmer,game programming,game technology,independent game developers conference,new game,nintendo development,online game development,pc game,playstation 2,playstation development,ps2,videogame,virtual reality,xbox game},
pages = {1--5},
title = {{Beyond Fa\c{c}ade: Pattern Matching for Natural Language Applications}},
url = {http://www.gamasutra.com/view/feature/6305/beyond\_fa\c{c}ade\_pattern\_matching\_.php?page=1},
year = {2011}
}

@article{Manning2014,
author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
file = {:home/adrian/current/licenta/bibliography/acl2014-corenlp.pdf:pdf},
journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
pages = {55--60},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://www.aclweb.org/anthology/P/P14/P14-5010},
year = {2014}
}

@article{Turing1950,
author = {Turing, Alan},
file = {:home/adrian/pub/4/2/licenta/bibliography/COMPUTING-MACHINERY-AND-INTELLIGENCE-Turing.pdf:pdf},
issn = {0026-4423},
journal = {Mind},
number = {236},
pages = {433--460},
title = {{Computing machinery and intelligence}},
url = {papers2://publication/uuid/E74CAAC6-F3DD-47E7-AEA6-5FB511730877},
volume = {59},
year = {1950}
}

@article{Lehmann2012,
abstract = {The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on theWeb using SemanticWeb and Linked Data technologies. The project extracts knowledge from 111 different language editions ofWikipedia. The largest DBpedia knowledge base which is extracted from the English edition ofWikipedia consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other 110Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties. The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the differentWikipedia editions to be combined. The project publishes regular releases of all DBpedia knowledge bases for download and provides SPARQL query access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases, the project maintains a live knowledge base which is updated whenever a page inWikipedia changes. DBpedia sets 27 million RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia data. Several hundred data sets on theWeb publish RDF links pointing to DBpedia themselves and thus make DBpedia one of the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and applications. Keywords:},
author = {Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mende, Pablo N. and Hellmann, Sebastian and Morsey, Mohamed and van Kleef, Patrick and Auer, Soren and Bizer, Christian},
doi = {10.3233/SW-140134},
file = {:home/adrian/current/licenta/bibliography/DBpedia2014.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
keywords = {knowledge extraction,linked data,multilingual knowledge bases,rdf,wikipedia},
pages = {1--5},
title = {{DBpedia – A Large-scale , Multilingual Knowledge Base Extracted from Wikipedia}},
url = {http://svn.aksw.org/papers/2013/SWJ\_DBpedia/public.pdf},
volume = {1},
year = {2012}
}

@article{Miller1990,
abstract = {WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.},
author = {Miller, George A. and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine J.},
doi = {10.1093/ijl/3.4.235},
isbn = {0950384614774577},
issn = {09503846},
journal = {International Journal of Lexicography},
number = {4},
pages = {235--244},
pmid = {15102489},
title = {{Introduction to wordnet: An on-line lexical database}},
volume = {3},
year = {1990}
}

@book{Fellbaum1998,
abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
author = {Fellbaum, Christiane},
booktitle = {British Journal Of Hospital Medicine London England 2005},
doi = {10.1139/h11-025},
isbn = {026206197X},
issn = {17508460},
number = {3},
pages = {423},
pmid = {21561289},
title = {{WordNet: An Electronic Lexical Database}},
url = {http://acl.ldc.upenn.edu/J/J99/J99-2008.pdf},
volume = {71},
year = {1998}
}

@article{Finlayson2011,
abstract = {JWI (the MIT Java Wordnet Interface) is a Java library for interfacing with Wordnet. JWI supports access to Wordnet versions 1.6 through 3.0, among other related Wordnet extensions. Wordnet is a freely and publicly available semantic dictionary of English, developed at Princeton University. JWI is written for Java 1.5.0 and has the package namespace edu.mit.jwi. The distribution does not include the Wordnet dictionary files; these can be downloaded from the Wordnet download site. This version of software is distributed under a license that makes it free to use for all purposes, as long as proper copyright acknowledgement is made. The javadoc API is posted online for your convenience. So is the version changelog. If you find JWI useful, have found a bug, or would like to request a new feature, please contact me.},
author = {Finlayson, Mark a.},
file = {:home/adrian/current/licenta/bibliography/edu.mit.jwi\_2.3.3\_manual.pdf:pdf},
pages = {1--10},
title = {{MIT Java Wordnet Interface (JWI) User's Guide}},
year = {2011}
}

@book{VanRijsbergen1979,
abstract = {Information retrieval is a wide, often loosely-defined term but in these pages I shall be concerned only with automatic information retrieval systems. Automatic as opposed to manual and information as opposed to data or fact. Unfortunately the word information can be very misleading. In the context of information retrieval (IR), information, in the technical meaning given in Shannon's theory of communication, is not readily measured (Shannon and Weaver1). In fact, in many cases one can...},
author = {{Van Rijsbergen}, C J},
booktitle = {Butterworths},
title = {{Information Retrieval, 2nd edition}},
url = {citeseer.ist.psu.edu/vanrijsbergen79information.html$\backslash$nhttp://www.dcs.gla.ac.uk/Keith/Preface.html},
year = {1979}
}

@article{Biaecki2012,
abstract = {Apache Lucene is a modern, open source search library designed to provide both relevant results as well as high performance. Furthermore, Lucene has undergone significant change over the years, starting as a one - person project to one of the leading search solutions available. Lucene is used in a vast range of applications from mobile devic es and desktops through Internet scale solutions. The evolution of Lucene has been quite dramatic at times, none more so than in the current release of Lucene 4.0. This paper presents both an overview of Lucene’s features as well as details on its commun ity development model, architecture and implementation, including coverage of its indexing and scoring capabilities.},
author = {Białecki, Andrzej and Muri, Robert and Ingersoll, Grant},
file = {:home/adrian/current/licenta/bibliography/lucene\_architecture.pdf:pdf},
isbn = {978-0-473-22026-6},
journal = {Proceedings of the SIGIR 2012 Workshop on Open Source Information Retrieval},
pages = {17--24},
title = {{Apache Lucene 4}},
url = {http://www.cs.otago.ac.nz/homepages/andrew/involvement/2012-SIGIR-OSIR.pdf?origin=publication\_detail\#page=22},
year = {2012}
}

@misc{StanfordTokenizer,
keywords = {StanfordTokenizer},
mendeley-tags = {StanfordTokenizer},
title = {{Stanford Tokenizer Documentation}},
howpublished={\url{http://nlp.stanford.edu/software/tokenizer.shtml}},
note= {Accessed: 2015-06-26},
url = {http://nlp.stanford.edu/software/tokenizer.shtml}
}

@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24\% accuracy on the Penn Treebank WSJ, an error reduction of 4.4\% on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
doi = {10.3115/1073445.1073478},
file = {:home/adrian/current/licenta/bibliography/stanford/tagging.pdf:pdf},
journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
pages = {252--259},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://dl.acm.org/citation.cfm?id=1073478},
year = {2003}
}

@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models. In this paper, we review our experience with constructing one such large annotated corpus-the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcus, Santorini, Marcinkiewicz - 1993 - Building a Large Annotated Corpus of English The Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {2},
pages = {313--330},
title = {{Building a Large Annotated Corpus of English: The Penn Treebank}},
url = {http://portal.acm.org/citation.cfm?id=972470.972475},
volume = {19},
year = {1993}
}

@article{Finkel2005,
abstract = {Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, andCRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9\% over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:home/adrian/current/licenta/bibliography/stanford/ner.pdf:pdf},
journal = {in Acl},
number = {1995},
pages = {363 -- 370},
title = {{Incorporating non-local information into information extraction systems by gibbs sampling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.8904},
year = {2005}
}

@article{Lee2013,
abstract = {We propose a new deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model's cluster output. The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision oriented, offer a powerful way to achieve both high precision and high recall. Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity. Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics.},
author = {Lee, Heeyoung and Chang, Angel and Peirsman, Yves and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan},
doi = {10.1162/COLI\_a\_00152},
file = {:home/adrian/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2013 - Deterministic coreference resolution based on entity-centric, precision-ranked rules.pdf:pdf;:home/adrian/current/licenta/bibliography/stanford/dcoref.pdf:pdf},
isbn = {9781608459858},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {4},
pages = {1--54},
title = {{Deterministic coreference resolution based on entity-centric, precision-ranked rules}},
url = {http://dx.doi.org/10.1162/COLI\_a\_00152$\backslash$nhttp://www.mitpressjournals.org/doi/pdf/10.1162/COLI\_a\_00152$\backslash$nhttp://www.mitpressjournals.org/toc/coli/0/ja$\backslash$nhttp://www.mitpressjournals.org/doi/abs/10.1162/COLI\_a\_00152},
volume = {39},
year = {2013}
}
